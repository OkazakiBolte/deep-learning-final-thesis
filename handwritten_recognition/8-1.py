#import numpy as np
import tensorflow as tf

# MNISTãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from tensorflow.keras.datasets import mnist
#from keras.datasets import mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()


#è¨“ç·´ãƒ‡ãƒ¼ã‚¿åˆã‚ã®ï¼“ã¤ã®å¯è¦–åŒ–
import numpy as np
import matplotlib.pyplot as plt
#%matplotlib inline #ã“ã®ã‚³ãƒãƒ³ãƒ‰ã¯ä½¿ãˆãªã„ã‚‰ã—ã„
#https://teratail.com/questions/54481

shuffle_index = np.random.permutation(60000)
x_train, y_train = x_train[shuffle_index], y_train[shuffle_index]


plt.figure(1, figsize=(19, 113))
plt.subplots_adjust(wspace=0.25)
plt.gray()
for id in range(50):
    plt.subplot(5, 10, id + 1)
    img = x_train[id, :, :]
    plt.pcolor(255 - img)
    # plt.text(22, 26, "%d" % y_train[id],
    #          color='cornflowerblue', fontsize=16)
    plt.xlim(0, 27)
    plt.ylim(27, 0)
    plt.xticks(color="None")
    plt.yticks(color="None")
    plt.tick_params(bottom=False,
                left=False,
                right=False,
                top=False)
plt.show()


"""
#ãƒ‡ãƒ¼ã‚¿ã®å¤‰å½¢
from tensorflow.python.keras.utils import np_utils

x_train = x_train.reshape(60000, 784)  # (A)
x_train = x_train.astype('float32')   # (B)
x_train = x_train / 255               # (C)
num_classes = 10
y_train = np_utils.to_categorical(y_train, num_classes)  # (D)

x_test = x_test.reshape(10000, 784)
x_test = x_test.astype('float32')
x_test = x_test / 255
y_test = np_utils.to_categorical(y_test, num_classes)

# ãƒªã‚¹ãƒˆ 8-1-(4)
#ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å®šç¾©
np.random.seed(1)
#numpy.random.seed(seed=ã‚·ãƒ¼ãƒ‰ã«ç”¨ã„ã‚‹å€¤) ã‚’ã‚·ãƒ¼ãƒ‰ (ç¨®) ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€
#ç™ºç”Ÿã™ã‚‹ä¹±æ•°ã‚’ã‚ã‚‰ã‹ã˜ã‚å›ºå®šã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚
#ä¹±æ•°ã‚’ç”¨ã„ã‚‹åˆ†æã‚„å‡¦ç†ã§ã€å†ç¾æ€§ãŒå¿…è¦ãªå ´åˆãªã©ã«ç”¨ã„ã‚‰ã‚Œã¾ã™ã€‚
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.optimizers import Adam

model = Sequential()
#ã€ŒSequentialã‚¯ãƒ©ã‚¹ã¯å…¥åŠ›ã¨å‡ºåŠ›ãŒå¿…ãšï¼‘ã¤ãšã¤ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹æˆã—ã‹å®šç¾©ã™ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã€‚
#ã¾ãŸã€ä¸­é–“ã®å±¤å†…ã§ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’åˆ†å²ã•ã›ã‚‹ã‚ˆã†ãªæ§‹æˆã‚‚ä½œã‚Œã¾ã›ã‚“ã€‚ï¼ˆå±¤ã®ç·šå½¢ã‚¹ã‚¿ãƒƒã‚¯æ§‹æˆï¼‰ã€
#https://sinyblog.com/deaplearning/keras_how_to/
model.add(Dense(16, input_dim=784, activation='relu'))
#éš ã‚Œå±¤ã®ãƒ¦ãƒ‹ãƒƒãƒˆæ•°16ã€å…¥åŠ›ã®æ¬¡å…ƒï¼Ÿã¯784ã€æ´»æ€§åŒ–é–¢æ•°ã¯ReLU
model.add(Dense(10, activation='softmax'))
#å‡ºåŠ›å±¤ã®ãƒ¦ãƒ‹ãƒƒãƒˆæ•°10ã€æ´»æ€§åŒ–é–¢æ•°ã¯ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹
model.compile(loss='categorical_crossentropy',
              optimizer=Adam(), metrics=['accuracy'])
#ã€Œlossã¨ã„ã†å¼•æ•°ãŒæå¤±é–¢æ•°ã§ã€åå‰ã‚’æ–‡å­—åˆ—ã§æŒ‡å®šã—ã¾ã™ã€
#ã€Œoptimizerã«ã¯æœ€é©åŒ–æ–¹æ³•ã‚’åå‰ã§æŒ‡å®šã—ã¾ã™ã€
#http://marupeke296.com/IKDADV_DL_No2_Keras.html
#Adam(adaptive moment estimation)ã¯2015å¹´ã«Kingmaã‚‰ãŒè€ƒæ¡ˆã—ãŸã€ç¢ºç‡çš„å‹¾é…æ³•ã‚’ã‚ˆã‚Šæ´—ç·´ã•ã›ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
#Adamã¯Wikipediaã®ã€Œç¢ºç‡çš„å‹¾é…é™ä¸‹æ³•ã€ã®ãƒšãƒ¼ã‚¸ã«ã‚‚èª¬æ˜ãŒã‚ã‚‹ã€‚
#ã€Œmetrics: è¨“ç·´æ™‚ã¨ãƒ†ã‚¹ãƒˆæ™‚ã«ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚Šè©•ä¾¡ã•ã‚Œã‚‹è©•ä¾¡é–¢æ•°ã®ãƒªã‚¹ãƒˆï¼
#ä¸€èˆ¬çš„ã«ã¯metrics=['accuracy']ã‚’ä½¿ã†ã“ã¨ã«ãªã‚Šã¾ã™ï¼ã€
#https://keras.io/ja/models/model/#compile

# ãƒªã‚¹ãƒˆ 8-1-(5)
#å­¦ç¿’
import time

startTime = time.time()
# shuffle_index = np.random.permutation(60000)
# x_train, y_train = x_train[shuffle_index], y_train[shuffle_index]
history = model.fit(x_train, y_train, epochs=100, batch_size=100,
                    verbose=1, validation_data=(x_test, y_test))  # (A)
score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
print("Computation time:{0:.3f} sec".format(time.time() - startTime))


# ãƒªã‚¹ãƒˆ 8-1-(6)
#äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã¨æ­£ç­”ç‡ã®æ™‚é–“ç™ºå±•ã®å¯è¦–åŒ–
plt.figure(1, figsize=(10, 4))
plt.subplots_adjust(wspace=0.5)

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='training', color='black')
plt.plot(history.history['val_loss'], label='test',
color='cornflowerblue')
plt.ylim(0, 10)
plt.legend()
plt.grid()
plt.xlabel('epoch')
plt.ylabel('loss')

plt.subplot(1, 2, 2)
plt.plot(history.history['acc'], label='training', color='black')
plt.plot(history.history['val_acc'],label='test', color='cornflowerblue')
plt.ylim(0, 1)
plt.legend()
plt.grid()
plt.xlabel('epoch')
plt.ylabel('acc')
plt.show()


# ãƒªã‚¹ãƒˆ 8-1-(7)
#96å€‹ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æ­£èª¤ã®è¡¨ç¤º
def show_prediction():
    n_show = 96
    y = model.predict(x_test)  # (A)
    plt.figure(2, figsize=(12, 8))
    plt.gray()
    for i in range(n_show):
        plt.subplot(8, 12, i + 1)
        x = x_test[i, :]
        x = x.reshape(28, 28)
        plt.pcolor(1 - x)
        wk = y[i, :]
        prediction = np.argmax(wk)
        plt.text(22, 25.5, "%d" % prediction, fontsize=12)
        if prediction != np.argmax(y_test[i, :]):
            plt.plot([0, 27], [1, 1], color='cornflowerblue', linewidth=5)
        plt.xlim(0, 27)
        plt.ylim(27, 0)
        plt.xticks([], "")
        plt.yticks([], "")

# -- ãƒ¡ã‚¤ãƒ³
show_prediction()
plt.show()


# ãƒªã‚¹ãƒˆ 8-1-(10)
# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¯è¦–åŒ–
#ğŸ‘‡layers[0]ã«ã™ã‚‹ã¨éš ã‚Œå±¤ã€layers[1]ã«ã™ã‚‹ã¨å‡ºåŠ›å±¤
#ğŸ‘‡get_weights()[0]ã«ã™ã‚‹ã¨é‡ã¿ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€get_weights()[1]ã«ã™ã‚‹ã¨ãƒã‚¤ã‚¢ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‚ç…§ã§ãã‚‹
parameter_num = 10 #ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°
w = model.layers[0].get_weights()[1]
#print(w.ndim)
#print(w.shape[0])
#print(w.shape[1])
print(w)

plt.figure(1, figsize=(12, 3)) #figureï¼ˆå…¨ä½“ï¼‰ã®å¤§ãã•ï¼Ÿ
plt.gray() # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ãƒ¢ãƒ¼ãƒ‰ã«ã™ã‚‹ã€‚
plt.subplots_adjust(wspace=0.35, hspace=0.5) #ã‚°ãƒ©ãƒ•é–“ã®ä½™ç™½ã®è¨­å®š
for i in range(parameter_num): # ğŸ‘ˆdo i = 0 , parameter_num - 1
    plt.subplot(2, 8, i + 1) #subplot(è¡Œæ•°, åˆ—æ•°, ãƒ—ãƒ­ãƒƒãƒˆç•ªå·)
    w1 = w[:, i] #
    w1 = w1.reshape(28, 28)
    plt.pcolor(-w1)
    plt.xlim(0, 27)
    plt.ylim(27, 0)
    plt.xticks([], "")
    plt.yticks([], "")
    plt.title("%d" % i)
# end 'for' Pythonã¯å­—ä¸‹ã’ã‚’æ­¢ã‚ã‚‹ã ã‘ã§ãƒ«ãƒ¼ãƒ—ãŒé–‰ã˜ã‚‹ã‚‰ã—ã„ï¼ŸğŸ˜…
plt.show()
"""
