\relax 
\@writefile{toc}{\contentsline {chapter}{概要}{i}\protected@file@percent }
\@writefile{toc}{\contentsline {part}{第I部\hspace  {1zw}理論}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {第1章}数学的準備}{2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\jsc@mpt }}
\@writefile{lot}{\addvspace {10\jsc@mpt }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}線形代数}{2}\protected@file@percent }
\newlabel{eq : matrix_product}{{1.1}{3}}
\newlabel{eq : dot_product}{{1.2}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}確率論}{4}\protected@file@percent }
\newlabel{eq : conditional_probability}{{1.3}{5}}
\newlabel{eq : bayes_theorem}{{1.4}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}情報理論}{6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces 二値確率変数のシャノン・エントロピー}}{6}\protected@file@percent }
\newlabel{fig : shannon-entropy}{{1.1}{6}}
\newlabel{eq : cross-entropy}{{1.5}{7}}
\@writefile{toc}{\contentsline {chapter}{\numberline {第2章}機械学習の基礎}{8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\jsc@mpt }}
\@writefile{lot}{\addvspace {10\jsc@mpt }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}教師あり学習}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}最適化}{9}\protected@file@percent }
\newlabel{eq : the-derivative-equals-to-zero}{{2.1}{10}}
\newlabel{enum : end-of-GD}{{2}{10}}
\newlabel{eq : GD}{{2.2}{10}}
\newlabel{enum : updating}{{3}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces 大域的最小と局所的最小}}{11}\protected@file@percent }
\newlabel{fig : mins}{{2.1}{11}}
\citation{bib : momentum-sdg}
\citation{bib : AdaGrad}
\newlabel{eq : momentum-sdg}{{2.3}{12}}
\newlabel{eq : adagrad}{{2.4}{12}}
\citation{bib : Adam}
\newlabel{eq : adam-m}{{2.5}{13}}
\newlabel{eq : adam-v}{{2.6}{13}}
\newlabel{eq : adam-m-hat}{{2.7}{13}}
\newlabel{eq : adam-v-hat}{{2.8}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}ベイズ推定}{13}\protected@file@percent }
\newlabel{eq : bayesian_inference}{{2.9}{13}}
\newlabel{eq : definition}{{2.10}{14}}
\newlabel{eq : iid}{{2.11}{14}}
\newlabel{eq : log}{{2.12}{14}}
\newlabel{eq : delta}{{2.13}{14}}
\newlabel{eq : expectation}{{2.14}{14}}
\newlabel{eq : flipping}{{2.15}{14}}
\@writefile{toc}{\contentsline {chapter}{\numberline {第3章}フィードフォーワード・ニューラルネットワーク}{16}\protected@file@percent }
\@writefile{lof}{\addvspace {10\jsc@mpt }}
\@writefile{lot}{\addvspace {10\jsc@mpt }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}アーキテクチャとニューラルネットワークの働き}{16}\protected@file@percent }
\newlabel{sec : アーキテクチャとニューラルネットワークの働き}{{3.1}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces 手書き数字の「５」}}{17}\protected@file@percent }
\newlabel{fig : handwritten-digit-five}{{3.1}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces ニューラルネットワークの例}}{17}\protected@file@percent }
\newlabel{fig : NN-example}{{3.2}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}入力層}{17}\protected@file@percent }
\citation{bib : uat}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces 件のニューラルネットワークに対するグラフ}}{18}\protected@file@percent }
\newlabel{fig : graph-1}{{3.3}{18}}
\newlabel{eq : design-matrix}{{3.1}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}隠れ層}{18}\protected@file@percent }
\newlabel{eq : relu}{{3.2}{18}}
\newlabel{eq : sigmoid}{{3.3}{19}}
\newlabel{eq : softmax}{{3.4}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}出力層}{19}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces 活性化関数とそれらの導関数のグラフ}}{20}\protected@file@percent }
\newlabel{fig : activation-functions}{{3.4}{20}}
\newlabel{eq : cost-function-mean-squared-error}{{3.5}{20}}
\newlabel{eq : cost-function-cross-entropy}{{3.6}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}誤差逆伝搬法}{21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}微分の連鎖律}{21}\protected@file@percent }
\newlabel{eq : chain-rule}{{3.7}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}誤差逆伝搬法を用いた学習}{21}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces 逆伝搬のパスを図3.2\@setref@ {}のグラフに書き加えたもの}}{22}\protected@file@percent }
\newlabel{fig : backpropagation}{{3.5}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}勾配消失問題}{22}\protected@file@percent }
\newlabel{subsec : vanishing-gradient}{{3.5.3}{22}}
\@writefile{toc}{\contentsline {chapter}{\numberline {第4章}畳み込みニューラルネットワーク}{23}\protected@file@percent }
\@writefile{lof}{\addvspace {10\jsc@mpt }}
\@writefile{lot}{\addvspace {10\jsc@mpt }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}畳み込み処理}{23}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces ２次元畳み込み処理の例}}{24}\protected@file@percent }
\newlabel{fig : convolution-1}{{4.1}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}ゼロパディングとストライド}{24}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces ２次元畳み込み処理の例}}{25}\protected@file@percent }
\newlabel{fig : zero_padding}{{4.2}{25}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}フィルターの効果}{25}\protected@file@percent }
\newlabel{eq : filterK}{{4.1}{25}}
\citation{bib : Geron}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}プーリング}{26}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces 最大プーリングの例}}{26}\protected@file@percent }
\newlabel{fig : pooling}{{4.3}{26}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}ドロップアウト}{26}\protected@file@percent }
\citation{bib : LeCun}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}畳み込みニューラルネットワークのアーキテクチャ}{27}\protected@file@percent }
\newlabel{sec : 畳み込みニューラルネットワークのアーキテクチャ}{{4.6}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces LeNet-5}}{27}\protected@file@percent }
\newlabel{fig : lenet-5}{{4.4}{27}}
\@writefile{toc}{\contentsline {part}{第II部\hspace  {1zw}実装}{28}\protected@file@percent }
\citation{bib : MNIST}
\@writefile{toc}{\contentsline {chapter}{\numberline {第5章}方法}{29}\protected@file@percent }
\@writefile{lof}{\addvspace {10\jsc@mpt }}
\@writefile{lot}{\addvspace {10\jsc@mpt }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}MNISTデータセット}{29}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}フィードフォーワード・ニューラルネットワークによる学習}{29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}ベースとなるフィードフォーワード・ニューラルネットワークの構築}{29}\protected@file@percent }
\newlabel{subsec : ベースとなるフィードフォーワード・ニューラルネットワークの構築}{{5.2.1}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces MNISTデータセットの50個のデータ}}{30}\protected@file@percent }
\newlabel{fig : 50-MNIST-data}{{5.1}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces 最初に構築したシンプルなニューラルネットワーク}}{30}\protected@file@percent }
\newlabel{fig : simple-fnn}{{5.2}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}活性化関数の比較}{31}\protected@file@percent }
\newlabel{subsec : 活性化関数の比較}{{5.2.2}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}コスト関数の比較}{31}\protected@file@percent }
\newlabel{subsec : コスト関数の比較}{{5.2.3}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}最適化アルゴリズムの比較}{31}\protected@file@percent }
\newlabel{subsec : 最適化アルゴリズムの比較}{{5.2.4}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.5}エポック数、バッチサイズの変更}{31}\protected@file@percent }
\newlabel{subsec : エポック数、バッチサイズの変更}{{5.2.5}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.6}隠れ層を２層に増やした場合の性能評価}{31}\protected@file@percent }
\newlabel{subsec : 隠れ層を２層に増やした場合の性能評価}{{5.2.6}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.7}隠れ層を３層以上に増やした場合の性能評価}{32}\protected@file@percent }
\newlabel{subsec : 隠れ層を３層以上に増やした場合の性能評価}{{5.2.7}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces 隠れユニット数が指数的に減少するニューラルネットワーク}}{32}\protected@file@percent }
\newlabel{fig : funnel}{{5.3}{32}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces $a$と$N$の組み合わせ}}{32}\protected@file@percent }
\newlabel{tab : aとNの組み合わせ}{{5.1}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.8}ドロップアウトの効果}{32}\protected@file@percent }
\newlabel{subsec : ドロップアウトによる効果}{{5.2.8}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}畳み込みニューラルネットワークによる学習}{33}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}ベースとなる畳み込みニューラルネットワークの構築}{33}\protected@file@percent }
\newlabel{subsec : ベースとなる畳み込みニューラルネットワークの構築}{{5.3.1}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces 畳み込み層が一つの畳み込みニューラルネットワーク}}{33}\protected@file@percent }
\newlabel{fig : simple-cnn}{{5.4}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}フィルターのサイズと枚数の比較}{34}\protected@file@percent }
\newlabel{subsec : フィルターのサイズと枚数の比較}{{5.3.2}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}プーリング層の追加}{34}\protected@file@percent }
\newlabel{subsec : プーリング層の追加}{{5.3.3}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.4}LeNet-5の実装}{34}\protected@file@percent }
\newlabel{subsec : LeNet-5の実装}{{5.3.4}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.5}畳み込みニューラルネットワークのアーキテクチャ}{34}\protected@file@percent }
\newlabel{subsec : 畳み込みニューラルネットワークのアーキテクチャ}{{5.3.5}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces 上図がネットワークA、下図がネットワークB}}{35}\protected@file@percent }
\newlabel{fig : nn-a-b}{{5.5}{35}}
\@writefile{toc}{\contentsline {chapter}{\numberline {第6章}結果}{36}\protected@file@percent }
\@writefile{lof}{\addvspace {10\jsc@mpt }}
\@writefile{lot}{\addvspace {10\jsc@mpt }}
\newlabel{chap : 結果}{{6}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}フィードフォーワード・ニューラルネットワークによる学習}{36}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}ベースとなるフィードフォーワード・ニューラルネットワークの構築}{36}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces 隠れ層が１層のニューラルネットワークのコスト関数、正解率の時間発展}}{36}\protected@file@percent }
\newlabel{fig : 2}{{6.1}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces 先頭$100$個のテストデータに対するモデルの予測}}{37}\protected@file@percent }
\newlabel{fig : prediction-1}{{6.2}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}活性化関数の比較}{37}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces 活性化関数をシグモイド関数に変えた場合の学習曲線}}{38}\protected@file@percent }
\newlabel{fig : 8}{{6.3}{38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}コスト関数の比較}{38}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces コスト関数を平均二乗誤差に変えた場合の学習曲線}}{39}\protected@file@percent }
\newlabel{fig : 9}{{6.4}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces コスト関数を平均二乗誤差とし、エポック数を増やした場合の学習曲線}}{39}\protected@file@percent }
\newlabel{fig : 11}{{6.5}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.4}最適化アルゴリズムの比較}{39}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces オプティマイザにAdamを採用した場合の学習曲線}}{40}\protected@file@percent }
\newlabel{fig : 10}{{6.6}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Adamを採用した場合の先頭$100$個のテストデータに対するモデルの予測}}{40}\protected@file@percent }
\newlabel{fig : predictions-2}{{6.7}{40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.5}エポック数、バッチサイズの変更}{41}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces エポック数、バッチサイズを変更したときの実装結果}}{41}\protected@file@percent }
\newlabel{tab : エポック数、バッチサイズの変更}{{6.1}{41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.6}隠れ層を２層に増やした場合の性能評価}{41}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces 第２隠れ層のユニット数を変化させたときの学習結果}}{42}\protected@file@percent }
\newlabel{tab : two-hidden-layers-1}{{6.2}{42}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces 第１隠れ層と第２隠れ層のユニット数を入れ替えたときの学習結果の比較}}{42}\protected@file@percent }
\newlabel{tab : two-hidden-layers-2}{{6.3}{42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.7}隠れ層を３層以上に増やした場合の性能評価}{42}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces 隠れユニット数を$1/a$倍ずつ減らしていった多層ニューラルネットワークの学習結果}}{43}\protected@file@percent }
\newlabel{tab : 隠れ層を３層以上に増やした場合の性能評価}{{6.4}{43}}
\@writefile{lot}{\contentsline {table}{\numberline {6.5}{\ignorespaces 「逆漏斗型」と「寸胴型」ネットワークによる学習結果}}{43}\protected@file@percent }
\newlabel{tab : 「逆漏斗型」と「寸胴型」ネットワークによる学習結果}{{6.5}{43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.8}ドロップアウトによる効果}{43}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces 学習が遅い場合の学習曲線}}{43}\protected@file@percent }
\newlabel{fig : 58}{{6.8}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces オプティマイザをAdamにして学習させたときの学習曲線}}{44}\protected@file@percent }
\newlabel{fig : 59}{{6.9}{44}}
\@writefile{lot}{\contentsline {table}{\numberline {6.6}{\ignorespaces ドロップアウト率に対する性能の実験結果}}{44}\protected@file@percent }
\newlabel{tab : ドロップアウト率に対する性能の実験結果}{{6.6}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces $p = 0.75,\nobreakspace  {}$エポック数$100$のときの学習曲線}}{44}\protected@file@percent }
\newlabel{fig : 66}{{6.10}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces $p = 0.75,\nobreakspace  {}$エポック数$1000$のときの学習曲線}}{45}\protected@file@percent }
\newlabel{fig : 63}{{6.11}{45}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}畳み込みニューラルネットワークによる学習}{45}\protected@file@percent }
\newlabel{sec : 畳み込みニューラルネットワークによる学習}{{6.2}{45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}ベースとなる畳み込みニューラルネットワークの構築}{45}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces $3 \times 3$のフィルター$1$枚による畳み込みニューラルネットワークの学習曲線}}{46}\protected@file@percent }
\newlabel{fig : 40}{{6.12}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.13}{\ignorespaces 先頭$100$個のテストデータに対する畳み込みニューラルネットワークの予測}}{47}\protected@file@percent }
\newlabel{fig : prediction-3}{{6.13}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}フィルターのサイズと枚数の比較}{47}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.7}{\ignorespaces フィルター数を増加させたときの実行結果}}{47}\protected@file@percent }
\newlabel{tab : フィルター数を増加させたときの実行結果}{{6.7}{47}}
\@writefile{lot}{\contentsline {table}{\numberline {6.8}{\ignorespaces フィルターサイズを変化させたときの実行結果}}{48}\protected@file@percent }
\newlabel{tab : フィルターサイズを変化させたときの実行結果}{{6.8}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}プーリング層の追加}{48}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.9}{\ignorespaces プーリングカーネルのサイズと対する実行結果}}{48}\protected@file@percent }
\newlabel{tab : プーリングカーネルのサイズと対する実行結果}{{6.9}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.4}LeNet-5の実装}{48}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.14}{\ignorespaces LeNet-5の学習曲線}}{49}\protected@file@percent }
\newlabel{fig : 68}{{6.14}{49}}
\@writefile{lot}{\contentsline {table}{\numberline {6.10}{\ignorespaces ドロップアウトを挿入したときの結果}}{49}\protected@file@percent }
\newlabel{tab : ドロップアウトを挿入したときの結果}{{6.10}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.5}畳み込みニューラルネットワークのアーキテクチャ}{50}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.11}{\ignorespaces ネットワークAとBについての実行結果}}{50}\protected@file@percent }
\newlabel{tab : ネットワークAとBについての実行結果}{{6.11}{50}}
\@writefile{toc}{\contentsline {chapter}{\numberline {第7章}考察}{51}\protected@file@percent }
\@writefile{lof}{\addvspace {10\jsc@mpt }}
\@writefile{lot}{\addvspace {10\jsc@mpt }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}「漏斗型」ニューラルネットワークにおけるパラメータ数の計算}{51}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces パラメータの数は$xy + y$}}{51}\protected@file@percent }
\newlabel{fig : num-params}{{7.1}{51}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}より精度の高いフィードフォーワード・ニューラルネットワークの構築}{52}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.3}より精度の高い畳み込みニューラルネットワークの構築}{52}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces 集大成のニューラルネットワーク}}{53}\protected@file@percent }
\newlabel{fig : Bolte-network}{{7.2}{53}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces 「集大成のニューラルネットワーク」の学習曲線}}{54}\protected@file@percent }
\newlabel{fig : Bolte-network-lc}{{7.3}{54}}
\@writefile{toc}{\contentsline {chapter}{\numberline {第8章}結論}{55}\protected@file@percent }
\@writefile{lof}{\addvspace {10\jsc@mpt }}
\@writefile{lot}{\addvspace {10\jsc@mpt }}
\@writefile{toc}{\contentsline {chapter}{\numberline {付録A}実装に用いたソースコード }{56}\protected@file@percent }
\@writefile{lof}{\addvspace {10\jsc@mpt }}
\@writefile{lot}{\addvspace {10\jsc@mpt }}
\newlabel{chap : source}{{A}{56}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}動作環境}{56}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.2}$1$隠れ層のフィードフォーワード・ニューラルネットワークのソースコード }{56}\protected@file@percent }
\newlabel{source : simple-fnn}{{A.1}{56}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.1}$1$隠れ層のニューラルネットワーク}{56}\protected@file@percent }
\newlabel{source : simple-fnn-3-3}{{A.2}{58}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.2}隠れ層を増やすために加えた変更}{58}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A.3}$1$隠れ層の畳み込みニューラルネットワークのソースコード}{58}\protected@file@percent }
\newlabel{source : simple-cnn}{{A.3}{58}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {A.3}$1$隠れ層の畳み込みニューラルネットワーク}{58}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{謝辞}{61}\protected@file@percent }
\bibcite{bib : Goodfellow}{1}
\bibcite{bib : momentum-sdg}{2}
\bibcite{bib : AdaGrad}{3}
\bibcite{bib : Adam}{4}
\bibcite{bib : uat}{5}
\bibcite{bib : Geron}{6}
\bibcite{bib : LeCun}{7}
\bibcite{bib : MNIST}{8}
\@writefile{toc}{\contentsline {chapter}{参考文献}{62}\protected@file@percent }
