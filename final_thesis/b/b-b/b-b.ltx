\documentclass[a4paper,11pt,oneside,openany,uplatex]{jsbook}
\input{/Users/bolte/Desktop/my_palace/programming/深層学習/卒業論文/settings.ltx}
\graphicspath{{/Users/bolte/Desktop/my_palace/programming/深層学習/卒業論文/fig/}}

\begin{document}
\chapter{結果}\label{chap : 結果}
  \section{フィードフォーワード・ニューラルネットワークによる学習}
    \subsection{ベースとなるフィードフォーワード・ニューラルネットワークの構築}
    節\ref{subsec : ベースとなるフィードフォーワード・ニューラルネットワークの構築}の結果である。付録\ref{chap : source}のソースコード\ref{source : simple-fnn}を実行したところ、次のような結果を得た。
   \begin{quote}
\begin{verbatim}
Test loss: 0.259500997513533
Test accuracy: 0.927500003576279
Computation time: 30.111 sec
\end{verbatim}
\end{quote}
テストデータに対する正解率は92.75\%であることがわかる。エポック数に対するコスト関数の値、正解率のグラフを出力させたところ、次の図\ref{fig : 2}のようになった。このようにコスト関数の値や汎化誤差の時間発展をグラフにしたものを\daiji{学習曲線}（learning curve）という。コスト関数の値はエポック数に対して単調に減少していることから、過学習は起きていないことがわかる。正解率も単調に増加している。
%１枚の画像
\begin{figure}[htbp]
\centering
\includegraphics[width=120mm]{lc/2.png}
\caption{隠れ層が１層のニューラルネットワークのコスト関数、正解率の時間発展}
\label{fig : 2}
\end{figure}
またパラメータの数を表示させたところ次のようになった。隠れ層（第$0$層）が\ty{dense (Dense)}、出力層（第$1$層）が\ty{dense_1 (Dense)}である。パラメータ数の合計は79,510で、すべて訓練できていた。
\begin{quote}
\begin{verbatim}
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense (Dense)                (None, 100)               78500
_________________________________________________________________
dense_1 (Dense)              (None, 10)                1010
=================================================================
Total params: 79,510
Trainable params: 79,510
Non-trainable params: 0
_________________________________________________________________
\end{verbatim}
\end{quote}


さらに初めの$100$個のテストデータに対するモデルの予測を表示させたものを次の図\ref{fig : prediction-1}に示す。各画像の右下にはネットワークの予測を付し、予測が誤りであったものは横棒で印をつけた。ここでは$100$個のうち$96$個が正解しているため、正解率92.75\%は正しく思われる。しかしながら、人間にとっても判別しづらい字はあるものの、$9$を$4$としたり比較的きれいに書かれている$2$を$7$と読み間違えている場合があり、精度は不十分なようである。
%１枚の画像
\begin{figure}[htbp]
\centering
\includegraphics[width=120mm]{fig-prediction-1.png}
\caption{先頭$100$個のテストデータに対するモデルの予測}
\label{fig : prediction-1}
\end{figure}

\subsection{活性化関数の比較}
節\ref{subsec : 活性化関数の比較}の結果は次のようになった。活性化関数をシグモイド関数にしただけで、正解率は$4.08\%$も下がった。また実行時間も$1.26\s$伸びてしまった。
   \begin{quote}
\begin{verbatim}
Test loss: 0.45101862847805
Test accuracy: 0.886699998378754
Computation time: 31.371 sec
\end{verbatim}
\end{quote}
学習曲線を図\ref{fig : 8}に示す。これと図\ref{fig : 2}を見比べると、エポック数がおよそ$40$までの範囲での学習が遅くなっていることがわかる。したがって精度の面でも計算速度の面でも、活性化関数はシグモイド関数よりもReLUの方が優れていることが確かめられた。
%１枚の画像
\begin{figure}[htbp]
\centering
\includegraphics[width=120mm]{lc/8.png}
\caption{活性化関数をシグモイド関数に変えた場合の学習曲線}
\label{fig : 8}
\end{figure}


\subsection{コスト関数の比較}
節\ref{subsec : コスト関数の比較}の結果は
   \begin{quote}
\begin{verbatim}
Test loss: 0.0511189009994268
Test accuracy: 0.706900000572205
Computation time: 29.949 sec
\end{verbatim}
\end{quote}
であった。実行時間が$0.162\s$だけ短くなったものの、正解率は$22.06\%$も下がって$70.69\%$となってしまった。学習曲線を図\ref{fig : 9}に示す。コスト関数の値はほとんど横這いでわずかに小さくなっているだけである。また正解率は図\ref{fig : 2}や図\ref{fig : 8}と比較して直線的であり、エポック数が足りていないだけで学習を続ければ性能が上がるのではないかと考え、エポック数を$5$倍の$500$にして実行させた。その学習曲線の結果を図\ref{fig : 11}に示す。正解率は図\ref{fig : 2}や図\ref{fig : 8}と同じように上昇している。しかし図\ref{fig : 2}では正解率$80\%$に到達するのに$4$か$5$エポックしか必要としないのに対し、図\ref{fig : 11}では同じ水準に達するのに$20$エポックほども必要であることが学習曲線から読み取れる。最終的な正解率は\verb|Test accuracy: 0.898300009965897|、実行時間は\verb|Computation time: 150.637 sec|であった。コスト関数が交差エントロピーの場合に比べておよそ$5$倍の実行時間を消費したにもかかわらず、正解率はおよそ$2.92\%$も下がっている。よってコスト関数としては、平均二乗誤差よりも交差エントロピーを用いる方が効率的に学習させることができるとわかった。
%１枚の画像
\begin{figure}[htbp]
\centering
\includegraphics[width=120mm]{lc/9.png}
\caption{コスト関数を平均二乗誤差に変えた場合の学習曲線}
\label{fig : 9}
\end{figure}
\begin{figure}[htbp]
\centering
\includegraphics[width=120mm]{lc/11.png}
\caption{コスト関数を平均二乗誤差とし、エポック数を増やした場合の学習曲線}
\label{fig : 11}
\end{figure}




\newpage
\subsection{最適化アルゴリズムの比較}
節\ref{subsec : 最適化アルゴリズムの比較}を実行したところ次の結果を得た。
\begin{quote}
\begin{verbatim}
Test loss: 0.106618618778884
Test accuracy: 0.97509999871254
Computation time: 31.240 sec
\end{verbatim}
\end{quote}
実行時間は$1.129\s$だけ長くなったものの、最適化アルゴリズムをAdamにしただけで、正解率はおよそ$4.76\%$も上がって$97.51\%$になった。図\ref{fig : 10}の学習曲線を見ると、数回のエポックで正解率が$90\%$に達していることがわかる。エポック数が$40$を超えたあたりから訓練データに関してコスト関数はほどんど$0$に、正解率はほとんど$100\%$に近い値を保持し続けている。このことから$60000$個の訓練データに関しては$40$回程度のエポックでほとんど学習が完了していることがわかる。%テストデータに関するコスト関数が単調に減少している
\begin{figure}[htbp]
\centering
\includegraphics[width=120mm]{lc/10.png}
\caption{オプティマイザにAdamを採用した場合の学習曲線}
\label{fig : 10}
\end{figure}
試しに先頭の$100$個のテストデータを予測させたところ、すべて正解であった（図\ref{fig : predictions-2}）。この精度であれば人間の認識とほとんど同じである（人間にも読み間違いはあるため、正解率$100\%$が理想ではない）。
%１枚の画像
\begin{figure}[htbp]
\centering
\includegraphics[width=120mm]{fig-predictions-2.png}
\caption{Adamを採用した場合の先頭$100$個のテストデータに対するモデルの予測}
\label{fig : predictions-2}
\end{figure}

\subsection{エポック数、バッチサイズの変更}
節\ref{subsec : エポック数、バッチサイズの変更}のようにエポック数とバッチサイズを変更した場合の実装結果を表\ref{tab : エポック数、バッチサイズの変更}にまとめて示す。節\ref{subsec : ベースとなるフィードフォーワード・ニューラルネットワークの構築}の基準となるモデルのものは太字で示した。また\verb|Test loss|と\verb|Accuracy|は小数点以下$4$桁までに四捨五入して表示してある。ミニバッチのサイズを$1000$に固定してエポック数を$10$倍ずつ増やしてゆくと、正解率が上がっていくことがわかる。また実行時間もエポック数に比例して増加している。ミニバッチのサイズが共通である場合にパラメータの更新回数はエポック数に比例することが現れている。またエポック数を$100$に固定してミニバッチのサイズを$10$倍ずつ増やしてゆくと、正解率は下がっていくことがわかる。
また実行時間はおおよそミニバッチが増えてゆくにつれて減ってゆく。パラメータの更新回数はミニバッチのサイズに反比例することに由来していると考えられる。
%１枚の表
\begin{table}[htbp]
\centering
\caption{エポック数、バッチサイズを変更したときの実装結果}
\label{tab : エポック数、バッチサイズの変更}
\begin{tabular}{rr||rrc}
\hline
エポック数 & ミニバッチのサイズ &コスト関数 & 正解率 & 実行時間 / sec \\
\hline\hline
10 & 1000 & 0.5729 & 0.8660 & 3.394 \\
\textbf{100} & \textbf{1000} & \textbf{0.2595} & \textbf{0.9275} & \textbf{30.111} \\
1000 & 1000 & 0.0981 & 0.9704 & 310.089 \\
\hline
100 & 100 & 0.0938 & 0.9715 & 76.001 \\
\textbf{100} & \textbf{1000} & \textbf{0.2595} & \textbf{0.9275} & \textbf{30.111} \\
100 & 10000 & 0.5691 & 0.8622 & 28.952 \\
100 & 100000 & 1.5860 & 0.6639 & 33.203 \\
\hline
\end{tabular}
\end{table}



\subsection{隠れ層を２層に増やした場合の性能評価}
%隠れ層を１層追加して、２層にして性能を比較した。ただしユニット数をいくつにすれば良いのかわからなかったため、第１層目の隠れ層のユニット数は$100$のまま、追加した第２層目の隠れ層のユニット数を$200,~100,~50,~25,~10$のように変化させたときのものを出力した。%次に第１層目、第２層目の隠れ層のユニット数をどちらも$10$として実験した。
%なお追加した第２層目の隠れ層の活性化関数は第１層目と同じくReLUとした。
第１層目の隠れ層のユニット数は$100$のまま、追加した第２層目の隠れ層のユニット数を$10,~25,~50,~100,~200$のように変化させたとき、学習の結果は表\ref{tab : two-hidden-layers-1}のようになった。総パラメータの数と実行時間はばらつきがあるものの、第２隠れ層のユニット数に比例しているように見える。コスト関数の最終的な値と正解率は第２隠れ層のユニット数に関わらずほとんど変わらない。

さらに第１隠れ層と第２隠れ層のユニット数を入れ替えて実験した。入れ替える前のものと比較したものを次の表\ref{tab : two-hidden-layers-2}に示す。このことから、出力層に近い方の隠れ層のユニット数が、入力層に近い方の隠れ層のユニット数よりも大きい場合、実行時間はかなり短縮されるが、正解率が$1\%$から$2\%$ほど落ちてしまうことがわかる。したがって精度の良いニューラルネットワークを構築したいならば、漏斗の形のように、出力層に進むにつれて隠れ層のユニット数を小さくしていくことが必要であるだろう。また出力層の１つ前の隠れ層のユニット数が、出力層のそれと等しくても精度が良いとは限らず、数倍程度大きいときに正解率が大きくなっていることがわかる。
%１枚の表
\begin{table}[htbp]
\centering
\caption{第２隠れ層のユニット数を変化させたときの学習結果}
\label{tab : two-hidden-layers-1}
\begin{tabular}{cr||rccc}
\hline
第１層 & 第２層 & 総パラメータ数 & コスト関数 & 正解率 & 実行時間 / sec \\
\hline\hline
100 & 10 & 79620 & 0.2084 & 0.9398 & 30.981 \\
100 & 25 & 81285 & 0.2073 & 0.9411 & 35.246 \\
100 & 50 & 83560 & 0.2120 & 0.9378 & 33.610 \\
100 & 100 & 89610 & 0.2173 & 0.9370 & 35.841 \\
100 & 200 & 100710 & 0.2250 & 0.9340 & 43.940 \\
\hline
\end{tabular}
\end{table}



%１枚の表
\begin{table}[htbp]
\centering
\caption{第１隠れ層と第２隠れ層のユニット数を入れ替えたときの学習結果の比較}
\label{tab : two-hidden-layers-2}
\begin{tabular}{cr||rccc}
\hline
第１層 & 第２層 & 総パラメータ数 & コスト関数 & 正解率 & 実行時間 / sec \\
\hline \hline
100 & 10 & 79620 & 0.2084 & 0.9398 & 30.981 \\
\textbf{10} & \textbf{100} & \textbf{9960} & \textbf{0.2807} & \textbf{0.9210} & \textbf{19.633} \\
100 & 25 & 81285 & 0.2073 & 0.9411 & 35.246 \\
\textbf{25} & \textbf{100} & \textbf{23235} & \textbf{0.2396} & \textbf{0.9310} & \textbf{22.033} \\
100 & 50 & 83560 & 0.2120 & 0.9378 & 33.610 \\
\textbf{50} & \textbf{100} & \textbf{45360} & \textbf{0.2322} & \textbf{0.9361} & \textbf{24.118} \\
100 & 200 & 100710 & 0.2250 & 0.9340 & 43.940 \\
\textbf{200} & \textbf{100} & \textbf{178110} & \textbf{0.2041} & \textbf{0.9406} & \textbf{56.645} \\
\hline
\end{tabular}
\end{table}







\newpage
\subsection{隠れ層を３層以上に増やした場合の性能評価}
隠れ層の数$N$と自然数$a$に対して隠れユニット数を層の順に$10 \times a^{N}, \ldots , 10 \times a$のように減らしていった「漏斗型」の多層ニューラルネットワークについて学習を行ったところ、表\ref{tab : 隠れ層を３層以上に増やした場合の性能評価}のような結果が得られた。$a = 2, ~ N = 6$の場合に最も高い正解率$96.60\%$を得ることができた。しかしパラメータ数が最も多いためか、実行時間は$192.090\s$と長いことがわかる。全体的に$a$の値が小さい、すなわち隠れユニット数が急激に減少しないネットワークで正解率は高くなるようだ。

さらに$a = 2 , ~ N = 6$の場合で隠れユニット数を逆転させた「逆漏斗型」ニューラルネットワークと隠れ層は$6$層で各層$210$のユニットを持つ「寸胴型」ニューラルネットワークについても学習を行わせたところ、表\ref{tab : 「逆漏斗型」と「寸胴型」ネットワークによる学習結果}の結果を得た。「逆漏斗型」の正解率は「漏斗型」に比べて$3 \%$程度も落ちている。実行時間は「漏斗型」が$192\s$ほどであったのに対し、「逆漏斗型」「寸胴型」はともに２分程度になっている。これはどちらもパラメータ数がおよそ半分程度に少なくなっているからだと考えられる。「寸胴型」の正解率$96.37\%$は「漏斗型」の正解率$96.60\%$とさほど変わらないが、それでも$0.23 \%$小さくなっている。したがって多層ニューラルネットワークにおいては、出力層に向かって隠れユニットの数が小さくなってゆけば正解率が高くなることがわかった。%隣り合う隠れ層の差が数倍程度であるときほど正解率は高くなるようだ。

%$a = 2, ~ N = 6$の場合は$a = 3, ~ N = 3$の場合に比べて、層の数が$3$だけ大きい。このことが全体の性能に影響を与えている可能性があり、公平な比較ができないと考えたので、隠れ層の数を同じにして$a = 2, ~ N = 3$の場合も確かめたところ、


%１枚の表
\begin{table}[htbp]
\centering
\caption{隠れユニット数を$1/a$倍ずつ減らしていった多層ニューラルネットワークの学習結果}
\label{tab : 隠れ層を３層以上に増やした場合の性能評価}
\begin{tabular}{cc||rrrr}
\hline
$a$ & $N$ & 総パラメータ数 & コスト関数 & 正解率 & 実行時間 / sec \\
\hline \hline
2 & 6 & 776030 & 0.1133 & 0.9660 & 192.090 \\
3 & 3 & 239380 & 0.1674 & 0.9504 & 67.056 \\
4 & 3 & 611810 & 0.1454 & 0.9558 & 134.113 \\
5 & 2 & 209310 & 0.1980 & 0.9422 & 58.341 \\
6 & 2 & 304870 & 0.1895 & 0.9451 & 72.679 \\
7 & 2 & 419730 & 0.1842 & 0.9472 & 94.051 \\
8 & 2 & 554490 & 0.1849 & 0.9465 & 116.425 \\
\hline
\end{tabular}
\end{table}

%１枚の表
\begin{table}[htbp]
\centering
\caption{「逆漏斗型」と「寸胴型」ネットワークによる学習結果}
\label{tab : 「逆漏斗型」と「寸胴型」ネットワークによる学習結果}
\begin{tabular}{c||rrrr}
\hline
アーキテクチャ     & 総パラメータ数 & コスト関数                  & 正解率    & 実行時間 / sec \\
\hline \hline
「逆漏斗型」 & 296150  & 0.2044                 & 0.9379 & 121.451    \\
「寸胴型」  & 388510  & 0.1178 & 0.9637 & 124.980   \\
\hline
\end{tabular}
\end{table}

\newpage
\subsection{ドロップアウトによる効果}
節\ref{subsec : ドロップアウトによる効果}のドロップアウト についての検証を行う。まず　ドロップアウト率$p = 0.25$で試したところ、図\ref{fig : 58}の学習曲線を得た。しかしテストデータによる正解率が訓練データによるそれよりも上回っており、テストデータの方が性能が良く現れている。このことは
学習があまり進んでおらず、数の少ないテストデータの学習の方が先に学習が進んでいること考えられる。学習を早めるため、オプティマイザにAdamを選ぶと図\ref{fig : 59}のようになって適切に学習ができていることがわかる。ただし見やすさのため図\ref{fig : 59}は縦軸の表示範囲を、左側のコスト関数のグラフについては$0$から$1,~$右側の正解率のグラフについては$0.8$から$1$のように変えてある。

%１枚の画像
\begin{figure}[htbp]
\centering
\includegraphics[width=120mm]{lc/58.png}
\caption{学習が遅い場合の学習曲線}
\label{fig : 58}
\end{figure}
\begin{figure}[htbp]
\centering
\includegraphics[width=120mm]{lc/59.png}
\caption{オプティマイザをAdamにして学習させたときの学習曲線}
\label{fig : 59}
\end{figure}

このようにオプティマイザをすべてAdamに変更して、ドロップアウト率を$ p = 0$（ドロップアウトをしない）$, ~ 0.25,~0.5,~0.75$と変化させて学習させると次の表\ref{tab : ドロップアウト率に対する性能の実験結果}の結果を得た。ドロップアウトを入れないよりも、$p = 0.25$で入れた方がわずかに性能が良くなっている。$p = 0.25$のときの性能が最も良いことがわかる。なお$p = 0.75$のとき学習曲線は図\ref{fig : 66}のようになり、学習が完了していないことがわかる。エポック数を大きくするなど対処しても、訓練データの誤差は小さくなってゆくが、テストデータの誤差が増加に転じてしまった（図\ref{fig : 63}）。これは訓練データのみに適合しすぎて過学習が起きているためである。ドロップアウト率が大きい時はうまく学習させることができなかった。したがってドロップアウト率は$p = 0.25 , ~ 0.5$程度で良いことがわかった。
%１枚の表
\begin{table}[htbp]
\centering
\caption{ドロップアウト率に対する性能の実験結果}
\label{tab : ドロップアウト率に対する性能の実験結果}
\begin{tabular}{r||rrrrr}
\hline
$p$ & エポック数 & ミニバッチのサイズ & コスト関数  & 正答率    & 実行時間 / sec \\
\hline \hline
0        & 100   & 1000      & 0.0939 & 0.9778 & 31.083     \\
0.25     & 100   & 1000      & 0.0781 & 0.9794 & 37.676     \\
0.50     & 100   & 100       & 0.1069 & 0.9776 & 95.312     \\
0.75     & 100   & 100       & 0.1403 & 0.9639 & 93.350   \\
\hline
\end{tabular}
\end{table}
%１枚の画像
\begin{figure}[htbp]
\centering
\includegraphics[width=120mm]{lc/66.png}
\caption{$p = 0.75,~$エポック数$100$のときの学習曲線}
\label{fig : 66}
\end{figure}
\begin{figure}[htbp]
\centering
\includegraphics[width=120mm]{lc/63.png}
\caption{$p = 0.75,~$エポック数$1000$のときの学習曲線}
\label{fig : 63}
\end{figure}



\newpage
   \section{畳み込みニューラルネットワークによる学習} \label{sec : 畳み込みニューラルネットワークによる学習}
\subsection{ベースとなる畳み込みニューラルネットワークの構築}
節\ref{subsec : ベースとなる畳み込みニューラルネットワークの構築}の結果を記す。学習をさせたところ、次の結果を得た。
\begin{quote}
\begin{verbatim}
Test loss: 0.327157819122076
Test accuracy: 0.909
Computation time:56.841 sec
\end{verbatim}
\end{quote}
単純なモデルであるが実行時間に$56.841\s$かかった。また正解率は$90.9\%$であり、これは節\ref{subsec : ベースとなるフィードフォーワード・ニューラルネットワークの構築}のフィードフォーワード・ニューラルネットワークの正解率$92.75\%$よりも低い。パラメータ数についての結果は以下のようになった。
\begin{quote}
\begin{verbatim}
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d (Conv2D)              (None, 28, 28, 1)         10
_________________________________________________________________
flatten (Flatten)            (None, 784)              0
_________________________________________________________________

dense (Dense)                (None, 10)                7850
=================================================================
Total params: 7,860
Trainable params: 7,860
Non-trainable params: 0
_________________________________________________________________
\end{verbatim}
\end{quote}
総パラメータ数は節\ref{subsec : ベースとなるフィードフォーワード・ニューラルネットワークの構築}のフィードフォーワード・ニューラルネットワークのそれよりに比べておよそ$1/10$に少なくなっているが、正解率が低いのはこの影響があると考えられる。学習曲線は次の図\ref{fig : 40}のようになった。コスト関数の値、正解率はそれぞれ単調に減少、増加しており、過学習はしていないことが確認できる。試しに初めの$100$個のテストデータに対するモデルの予測を表示させたものを次の図\ref{fig : prediction-3}に示す。ここでは$100$個のうち$95$個が正解している。図\ref{fig : prediction-1}と共通して間違えているものが３つあるのが確認できる。$1$を$3$と間違えていたり、$9$を$4$と答えている例がある。これらの数字は字形大まかに似ていて、たった１枚のフィルターで学習させたため仕方ないが、まだ精度は不十分である。しかし$3 \times 3$のフィルター$1$枚だけでも正解率は$9$割りを超えており、フィルター数を増やしたりプーリング層を加えたりすることでさらに改善されることが期待できる。


%１枚の画像
\begin{figure}[htbp]
\centering
\includegraphics[width=120mm]{lc/40.png}
\caption{$3 \times 3$のフィルター$1$枚による畳み込みニューラルネットワークの学習曲線}
\label{fig : 40}
\end{figure}

%１枚の画像
\begin{figure}[htbp]
\centering
\includegraphics[width=120mm]{fig-prediction-3.png}
\caption{先頭$100$個のテストデータに対する畳み込みニューラルネットワークの予測}
\label{fig : prediction-3}
\end{figure}

\newpage
\subsection{フィルターのサイズと枚数の比較}
節\ref{subsec : フィルターのサイズと枚数の比較}の結果を記す。
まずフィルターのサイズはすべて$3 \times 3$のまま、フィルター数を$2,~3,~4,~5,~6$のように変えて学習させたときの結果を次の表\ref{tab : フィルター数を増加させたときの実行結果}に示す。パラメータ数はフィルター数におおよそ比例している。また正解率はこの状況においてはわずかに増加していることがわかる。
%１枚の表
\begin{table}[htbp]
\centering
\caption{フィルター数を増加させたときの実行結果}
\label{tab : フィルター数を増加させたときの実行結果}
\begin{tabular}{c|rrrr}
\hline
フィルター数 & 総パラメータ数 & コスト関数 & 正解率 & 実行時間 / sec \\
\hline \hline
2 & 15710 & 0.3385 & 0.9029 & 65.392 \\
3 & 23560 & 0.3044 & 0.9145 & 67.026 \\
4 & 31410 & 0.3069 & 0.9144 & 72.644 \\
5 & 39260 & 0.3074 & 0.9159 & 77.334 \\
6 & 47110 & 0.3026 & 0.9144 & 78.914 \\
\hline
\end{tabular}
\end{table}

またフィルター数は$1$のまま、フィルターのサイズを$4 \times 4,~5 \times 5, ~6\times 6$と変化させたとき、実行結果は表\ref{tab : フィルターサイズを変化させたときの実行結果}のようになった。フィルターサイズを大きくさせても、正解率はほとんど同じで、実行時間だけが延びることがわかる。したがってフィルターはサイズが小さいものを用いれば、効率が良いことが実験的にわかった。

%１枚の表
\begin{table}[htbp]
\centering
\caption{フィルターサイズを変化させたときの実行結果}
\label{tab : フィルターサイズを変化させたときの実行結果}
\begin{tabular}{c|rrrr}
\hline
フィルターサイズ & 総パラメータ数 & コスト関数 & 正解率 & 実行時間 / sec \\
\hline \hline
$4 \times 4$ & 7867 & 0.3218 & 0.9069 & 97.402 \\
$5 \times 5$ & 7876 & 0.3554 & 0.8984 & 139.740 \\
$6 \times 6$ & 7887 & 0.3272 & 0.9084 & 191.790 \\
\hline
\end{tabular}
\end{table}

\subsection{プーリング層の追加}
 節\ref{subsec : プーリング層の追加}の検証結果を記す。まずプーリングカーネルのサイズが$2 \times 2$の最大プーリングを行ったときの結果は次のようになった。これを見ると、プーリングを行うだけでは精度が悪いことがわかる。
 \begin{quote}
 \begin{verbatim}
Test loss: 0.823955835819244
Test accuracy: 0.8323
Computation time:8.764 sec
 \end{verbatim}
 \end{quote}
 平均プーリングを行った結果は次のようになった。実行時間は変わらないものの、正解率がわずかに下がっていることがわかる。またコスト関数の値も最大プーリングのときよりも大きい。これらのことから最大プーリングの方が効率がよいと考えられる。
 \begin{quote}
 \begin{verbatim}
Test loss: 1.0449662100791932
Test accuracy: 0.8018
Computation time:8.782 sec
 \end{verbatim}
 \end{quote}

 さらにプーリングカーネルのサイズを変えて学習を行うと、表\ref{tab : プーリングカーネルのサイズと対する実行結果}のような結果を得た。プーリングカーネルのサイズが大きくなると出力の特徴マップが縮小するのでパラメータ数が減少している。また正解率は大きく減少することがわかる。これも特徴マップが小さくなるためである。したがってプーリングを行うときは、小さいプーリングカーネルで最大プーリングを行うのがよいことがわかった。
%１枚の表
\begin{table}[htbp]
\centering
\caption{プーリングカーネルのサイズと対する実行結果}
\label{tab : プーリングカーネルのサイズと対する実行結果}
\begin{tabular}{c||rrrr}
\hline
プーリングカーネル & 総パラメータ数 & コスト関数 & 正解率 & 実行時間 / sec \\
\hline \hline
$2 \times 2$ & 1970 & 0.8240 & 0.8323 & 8.764 \\
$3 \times 3$ & 820 & 1.1241 & 0.7773 & 8.079 \\
$4 \times 4$ & 500 & 1.4857 & 0.6395 & 8.815 \\
\hline
\end{tabular}
\end{table}



 \subsection{LeNet-5の実装}
LeNet-5を実装したところ、次の結果を得た。
  \begin{quote}
 \begin{verbatim}
Test loss: 0.186557103292644
Test accuracy: 0.9436
Computation time:256.474 sec
 \end{verbatim}
 \end{quote}


 実行時間は$4$分半ほどかかったものの、畳み込み層、プーリング層が１層のときに比べてかなり精度が良くなっている。学習曲線は図\ref{fig : 68}のようになり、過学習は起きていないことがわかる。

 また畳み込み層C5とF6の間、F6と出力層の間、そのどちらにも$p = 0.25$のドロップアウトを挿入した場合について性能を出力させると次の表\ref{tab : ドロップアウトを挿入したときの結果}のようになった。両方に挿入すると性能が$0.63\%$落ちた。また片方のみに挿入すれば、性能が若干上がっている。これは出力層から遠い層のものほどユニットが多いため、効率的にドロップアウトを用いられているためと考えられる。しかし多くの層の間でドロップアウトを行うと学習に時間がかかり、効率が悪くなる。

  %１枚の画像
\begin{figure}[htbp]
\centering
\includegraphics[width=120mm]{lc/68.png}
\caption{LeNet-5の学習曲線}
\label{fig : 68}
\end{figure}

 %１枚の表
\begin{table}[htbp]
\centering
\caption{ドロップアウトを挿入したときの結果}
\label{tab : ドロップアウトを挿入したときの結果}
\begin{tabular}{c||rrr}
\hline
ドロップアウトを挿入した場所 & コスト関数  & 正解率    & 実行時間 / sec \\
\hline \hline
(挿入しない) & 0.1866 &  0.9436 & 256.474 \\
C5—F6間         & 0.1744 & 0.9473 & 258.315    \\
F6—出力層間        & 0.1876 & 0.9437 & 264.223    \\
両方             & 0.2120 & 0.9373 & 256.262   \\
\hline
\end{tabular}
\end{table}


 \newpage
 \subsection{畳み込みニューラルネットワークのアーキテクチャ}
 節\ref{subsec : 畳み込みニューラルネットワークのアーキテクチャ}の２つのネットワークAとBについて学習を行うと表\ref{tab : ネットワークAとBについての実行結果}の結果を得た。どちらの畳み込みネットワークでもさほど変わらない正解率を得たが、ネットワークBの実行にはAの約$6.5$倍にあたる$34$分もかかってしまった。したがって畳み込みニューラルネットワークでは出力層に向かってフィルター数の増加していく「末広がり」の形をしたアーキテクチャで効率よく学習させることができるとわかった。

%１枚の表
\begin{table}[htbp]
\centering
\caption{ネットワークAとBについての実行結果}
\label{tab : ネットワークAとBについての実行結果}
\begin{tabular}{c||rrrr}
\hline
ネットワーク & 総パラメータ数 & コスト関数  & 正解率    & 実行時間 / sec \\
\hline \hline
A      & 74122   & 0.2199 & 0.9353 & 311.363    \\
B      & 260122  & 0.2121 & 0.9396 & 2041.787  \\
\hline
\end{tabular}
\end{table}


























\end{document}

