\documentclass[a4paper,11pt,oneside,openany,uplatex]{jsbook}
\input{/Users/bolte/Desktop/my_palace/programming/深層学習/卒業論文/settings.ltx}
\graphicspath{{/Users/bolte/Desktop/my_palace/programming/深層学習/卒業論文/fig/}}

\begin{document}
\chapter{方法}
%\red{「OZ10 5章。実際に君が何を行ったのか全然書いてない。まずそれを詳しく説明して、その後に実装方法が来るべき。」}
\cyan{
手書きの数字のデータセットであるMNISTデータセットを題材にして、実際にフィードフォーワード・ニューラルネットワークと畳み込みニューラルネットワークの構築を行なった。
}

\cyan{
簡単なフィードフォーワード・ニューラルネットワークを実装したのち、活性化関数、コスト関数、最適化アルゴリズム、隠れ層の数などの部品を組み換えてプログラムを実行させ、それらがどのように性能に影響を与えるかを研究した。
}
\cyan{
同様に簡単な畳み込みニューラルネットワークを実装し、畳み込みフィルターのサイズやプーリングカーネルの種類を変化させてその結果を比較した。さらにLeNet-5を実際に構築して性能を確認した。最後に出力層に向かって、畳み込みフィルターと隠れ層のユニット数を増加させたときと減少させたときで性能を評価し、どちらが応用に向いているのかを判定した。
}

  \section{MNISTデータセット}
\textsf{MNIST}\daiji{データセット}（Mixed National Institute of Standards and Technology database）\cyan{\cite{bib : MNIST}}は60000個の訓練データと10000個のテストデータからなる、手書きの数字のデータセットである。それぞれのデータは手書き数字の画像と、表している数字の正解のラベルからなる。各画像は$28 \times 28 $ピクセルで、各ピクセルは$0$（白）から$255$（黒）までの値でその明度を表している。MNISTデータセットは、最初の60000個が訓練セット、後ろの10000個がテストセットにはじめから分かれている。このデータセットは非常によく使われており、新しい分類アルゴリズムが登場するとまずMNISTデータセットで性能を測るので、機械学習%における「Hello World」と言われている。
\cyan{の初心者が必ず触れる題材であると言われている。}図\ref{fig : 50-MNIST-data}は、MNISTデータセットの訓練セットからランダムに取り出された$50$個のデータである。各画像の右下には正解のラベルの値も付してある。
%１枚の画像
%\begin{comment}
\begin{figure}[htbp]
\centering
\includegraphics[width=150mm]{fig-50-MNIST-data.png}
\caption{MNISTデータセットの50個のデータ}
\label{fig : 50-MNIST-data}
\end{figure}
%\end{comment}



   \section{フィードフォーワード・ニューラルネットワークによる学習}

%   あああああああああああ
\subsection{ベースとなるフィードフォーワード・ニューラルネットワークの構築}\label{subsec : ベースとなるフィードフォーワード・ニューラルネットワークの構築}
% ベースとなるニューラルネットワーク
% epoch v.s. error rate を示す
% 重みの観察
%TensorFlowに付属のKerasを用いて、比較対象となる隠れ層が$1$層のフィードフォーワード・ニューラルネットワークのプログラムを作成した。%\cyan{全体的なプログラムの構造を説明する。まずMNISTデータセットをインポートしたのち取り扱いやすいようにデータの配列などを変換した。その後ニューラルネットワークのアーキテクチャを定義し、実際にデータを用いて訓練、その結果を計算させ、最後に結果を表示させた。そのソースコードを付録\ref{chap : source}の\ref{source : simple-fnn}に示す。}
\cyan{活性化関数や隠れ層の数などを変化させたときにニューラルネットワークの性能にどのような違いが生じるかを試したかったので、できるだけシンプルな、隠れ層が$1$層のフィードフォーワード・ニューラルネットワークのプログラムを作成した。機械学習のためのオープンソースライブラリであるKerasを用いてソースコードを書いた。}

\cyan{まずMNISTデータセットの各画像データはサイズが$28 \times 28$の２次元の配列であるが、これを$784$次元のベクトルに変換した。各ピクセル値は$0$から$255$までの整数であるが、これを$0$から$1$までの値に変換した。またデータのラベルに対応したワンホットベクトルを作成しておいた。次に学習をさせるニューラルネットワークを構築した。各ユニットが次の層のすべてのユニットに影響する全結合型ニューラルネットワークとし、隠れ層は$1$層でそのユニット数は$100$としておいた。入力層は$784$ユニット、出力層は$10$個の分類をするので$10$ユニットである。図にすると図\ref{fig : simple-fnn}のようになる。}
\begin{figure}[htbp]
\centering
\includegraphics[width=100mm]{fig-simple-fnn.png}
\caption{最初に構築したシンプルなニューラルネットワーク}
\label{fig : simple-fnn}
\end{figure}
\cyan{入力層から隠れ層への活性化関数はReLUを、隠れ層から出力層への活性化関数はソフトマックス関数とした。コスト関数は交差エントロピーを採用し、出力とラベルのワンホットベクトルからこれを計算した。誤差逆伝搬によりコスト関数の勾配から、入力層と隠れ層、隠れ層と出力層の間の重みパラメータおよび隠れ層、出力層の各ユニットについてのバイアスのパラメータの更新を行なった。ただし最適化アルゴリズムは確率的勾配降下法とし、ミニバッチのサイズは$1000$としておいた。この処理を$60000 / 1000 = 60$回繰り返した。ただしミニバッチは確率的に選ばれるため、この反復によって$60000$個の訓練データすべてが用いられるわけではない。次に$10000$個のテストデータを用いて予測を行い、正解率を算出した。ここまでの反復を$1$エポックという。エポック数は$100$として、合計で$100 \times 60 = 6000$回のパラメータ更新が行われるようにした。初期設定を箇条書きにまとめると次のようになる。これ以降ではこれらを少しずつ変化させて性能を比較していった。
\begin{itemize}
  \item 隠れ層：層の数は$1$でユニット数は$100$
  \item 入力層から隠れ層への活性化関数：ReLU
  \item 隠れ層から出力層への活性化関数：ソフトマックス関数
  \item コスト関数：交差エントロピー
  \item 最適化アルゴリズム：確率的勾配降下法（SGD）
  \item ミニバッチのサイズ：$1000$
  \item エポック数：$100$
\end{itemize}
プログラムのソースコードとそのより詳しい説明は付録\ref{chap : source}の\ref{source : simple-fnn}に示した。}
%\subsection{訓練データをシャッフルすることの効果}

\subsection{活性化関数の比較}\label{subsec : 活性化関数の比較}
%\ty{(3-3)}の活性化関数を\ty{relu}から\ty{sigmoid}に変えてプログラムを実行し性能を比較した。
活性化関数をReLUからシグモイド関数に変えてプログラムを実行し性能を比較した。
\subsection{コスト関数の比較}\label{subsec : コスト関数の比較}
%\ty{(3-5)}のコスト関数を\ty{categorical_crossentropy}から\ty{mean_squared_error}に変えてプログラムを実行し性能を比較した。
コスト関数を交差エントロピーから平均二乗誤差に変えてプログラムを実行し性能を比較した。
\subsection{最適化アルゴリズムの比較}\label{subsec : 最適化アルゴリズムの比較}
%\ty{(3-5)}のオプティマイザを\ty{sgd}（確率的勾配降下法）から\ty{Adam}に変えてプログラムを実行し性能を比較した。
最適化アルゴリズムを確率的勾配降下法からAdamに変えて性能を比較した。

\subsection{エポック数、バッチサイズの変更}\label{subsec : エポック数、バッチサイズの変更}
エポック数を$10,~1000$に、バッチサイズを$100,~10000,~100000$にそれぞれ変更したときの結果を表示させ、それらを比較した。


\subsection{隠れ層を２層に増やした場合の性能評価}\label{subsec : 隠れ層を２層に増やした場合の性能評価}
隠れ層を１層追加して、２層にして性能を比較した。ただしユニット数をいくつにすれば良いのかわからなかったため、第１層目の隠れ層のユニット数は$100$のまま、追加した第２層目の隠れ層のユニット数を$10,~25,~50,~100,~200$のように変化させたときのものを出力した。逆に第２の隠れ層のユニット数を$100$にしたまま、第１の隠れ層のユニット数を同じように変化させて比較した。%次に第１層目、第２層目の隠れ層のユニット数をどちらも$10$として実験した。
なお追加した第２層目の隠れ層の活性化関数は第１層目と同じくReLUとした。
\subsection{隠れ層を３層以上に増やした場合の性能評価} \label{subsec : 隠れ層を３層以上に増やした場合の性能評価}
節\ref{subsec : 隠れ層を２層に増やした場合の性能評価}を実行することによって、出力層に向かうにつれてユニット数を少なくしてゆく「漏斗型」のフィードフォーワード・ニューラルネットワークが、精度良く数字を予測することを経験的に発見した。そこでネットワークをより深くするとき、どのように細くしてゆくのがいいかを検証した。具体的には$a$をある自然数、$n_{\mathrm{classes}} = 10$をクラスの数、隠れ層の数を$N$としたとき（入力層と出力層も合わせれば全体で$\qty( N + 2 )$層のネットワークになる）、隠れ層のユニット数を順方向に$ 10 \times a^{N}, ~10 \times a^{N - 1}  ,~  \ldots ,~ 10 \times a^{2}  ,~ 10 \times a$のように、指数関数的に減らしていくことを考えた（図\ref{fig : funnel}）。ただし入力の次元が$784$であるから第１隠れ層のユニット数がそれよりも小さくなるように、$n_{\mathrm{classes}} = 10$と$a$の値を決めたのち$ 10 \times a^{N} < 784$を満たす最大の隠れ層数$N$について検証した。そのような数の組み合わせは表\ref{tab : aとNの組み合わせ}のように$7$通りあるので、それらを全て試した。ただし隠れ層の活性化関数はすべてReLUとした。
\cyan{またソースコードの上で隠れ層を増やすのには反復文を用いて手間を省く工夫をした。具体的には付録\ref{chap : source}のソースコード\ref{source : simple-fnn-3-3}のように変更した。}

比較のため、隠れユニット数を順方向に$ 10 \times a^1, ~10 \times a^{2}  ,~  \ldots ,~ 10 \times a^{N}$のように増加させていった「逆漏斗型」のニューラルネットワークの性能を$a = 2 , ~ N = 6$の場合に測定した。それとは別に、ユニット数が$210$で一定で層数$6$の「寸胴型」ニューラルネットワーク（総隠れユニット数は$210 \times 6 = \sum_{k = 1}^{6} 2^{k} = 1260$で同じになる）についても性能を測定した。


\begin{figure}[htbp]
\centering
\includegraphics[width=120mm]{fig-funnel.png}
\caption{隠れユニット数が指数的に減少するニューラルネットワーク}
\label{fig : funnel}
\end{figure}

%１枚の表
\begin{table}[htbp]
\centering
\caption{$a$と$N$の組み合わせ}
\label{tab : aとNの組み合わせ}
\begin{tabular}{c||c|c|c|c|c|c|c}
\hline
$a$ & 2 & 3 & 4 & 5 & 6 & 7 & 8 \\
\hline
$N$ & 6 & 3 & 3 & 2 & 2 & 2 & 2 \\
\hline
\end{tabular}
\end{table}



\subsection{ドロップアウトの効果} \label{subsec : ドロップアウトによる効果}
ドロップアウトは畳み込みニューラルネットワークで用いられる手法であるが、その中でも出力層に近くにある、部分的にフィードフォーワード・ニューラルネットワークとみなせる部分で用いられるため、節\ref{subsec : ベースとなるフィードフォーワード・ニューラルネットワークの構築}のニューラルネットワークを用いて検証を行った。

節\ref{subsec : ベースとなるフィードフォーワード・ニューラルネットワークの構築}のベースとなるニューラルネットワークの隠れ層と出力層の間にドロップアウト層を挿入し、ドロップアウトの割合を$ p = 0$（ドロップアウトをしない）$, ~ 0.25,~0.5,~0.75$のように変化させたときの出力をみた。ただし収束を早めるため最適化アルゴリズムはAdamを採用し、エポック数、ミニバッチのサイズはドロップアウト率に合わせて変化させた。














   \section{畳み込みニューラルネットワークによる学習}
\subsection{ベースとなる畳み込みニューラルネットワークの構築}\label{subsec : ベースとなる畳み込みニューラルネットワークの構築}
% ベースとなるニューラルネットワーク
% epoch v.s. error rate を示す
% 重みの観察
%比較対象となる畳み込みニューラルネットワークを構築した。隠れ層はサイズが$3 \times 3$の$1$枚のフィルター層とし、このフィルターを訓練した。
%エポック数は$20$バッチサイズは$1000$とした。\cyan{全体的なプログラムの構造はフィードフォーワード・ニューラルネットワークのときとさほど変わらず、データの変形の仕方、用いた隠れ層の種類などを変えただけである。その}ソースコードを付録\ref{chap : source}のソースコード\ref{source : simple-cnn}に示す。
\cyan{隠れ層を畳み込み層、プーリング層などに変化させたときに畳み込みニューラルネットワークの性能にどのような違いが生じるかを試したかったので、できるだけシンプルな、隠れ層が$1$層の畳み込みニューラルネットワークのプログラムを作成した。エポック数は$20,~$ミニバッチのサイズは$1000$とした。}

\cyan{まずMNISTデータセットの各画像データをそのまま扱うため、これを$60000 \times 28 \times 28$の配列に変換した。各ピクセル値は$0$から$255$までの整数であるが、これを$0$から$1$までの値に変換した。またデータのラベルに対応したワンホットベクトルを作成しておいた。次に学習をさせるニューラルネットワークを構築した。各ユニットが次の層のすべてのユニットに影響する全結合型ニューラルネットワークとし、隠れ層は１枚のフィルターを持つ$1$層の畳み込み層とし、そのフィルターのサイズは$3 \times 3$としておいた。畳み込み層の出力サイズが入力と変わらないように、適宜ゼロパディングを行っておいた。入力層から隠れ層への活性化関数はReLUを用いた。この隠れ層の出力のサイズは$1000$（ミニバッチのサイズ）$ \times 1$（フィルターの数）$ \times 28 \times 28$（画像のサイズ）の４次元であるが、次の出力層に入れるため、これを$1000 \times (1 \cdot 28 \cdot 28)$の２次元に変換する必要があった（Flatten層）。入力のデータが$1$枚のときの、この畳み込みニューラルネットワークの概念図を図\ref{fig : simple-cnn}に示す。}
%１枚の画像
\begin{figure}[htbp]
\centering
\includegraphics[width=120mm]{fig-simple-cnn.png}
\caption{畳み込み層が一つの畳み込みニューラルネットワーク}
\label{fig : simple-cnn}
\end{figure}
\cyan{隠れ層から出力層への活性化関数はソフトマックス関数とした。最適化アルゴリズムは確率的勾配降下法とし、エポック数は$20$として、合計で$20 \times 60 = 1200$回のパラメータ更新が行われるようにした。ここでパラメータはフィルターの$3 \times 3 = 9$個の値と、全体に加える$1$つのバイアスである。初期設定を箇条書きにまとめると次のようになる。これ以降ではこれらを少しずつ変化させて性能を比較していった。
\begin{itemize}
  \item 隠れ層：畳み込み層
   \begin{itemize}
     \item フィルターのサイズ：$3 \times 3$
     \item フィルターの数：$1$
   \end{itemize}
  \item 入力層から隠れ層への活性化関数：ReLU
  \item 隠れ層から出力層への活性化関数：ソフトマックス関数
  \item コスト関数：交差エントロピー
  \item 最適化アルゴリズム：確率的勾配降下法（SGD）
  \item ミニバッチのサイズ：$1000$
  \item エポック数：$20$
\end{itemize}
プログラムのソースコードとそのより詳しい説明は付録\ref{chap : source}の\ref{source : simple-fnn}に示した。}
%\subsection{訓練データをシャッフルすることの効果}

\subsection{フィルターのサイズと枚数の比較} \label{subsec : フィルターのサイズと枚数の比較}
$3 \times 3$のフィルター数を$1$枚ずつ$6$枚まで増やし、学習の結果を出力させた。またフィルター数は$1$枚に固定して、そのサイズを$4 \times 4,~5 \times 5, ~6\times 6$と変化させてその結果を表示させた。

\subsection{プーリング層の追加} \label{subsec : プーリング層の追加}
次に畳み込み層の代わりに最大プーリング層を通して実行結果をみた。プーリングカーネルのサイズは$2 \times 2$
%であり、畳み込み層の定義を\ty{model.add(MaxPooling2D(pool_size=(2, 2)))}に変えることで実現した。
とした。また平均プーリングを行ってどのような差異が現れるか確認した。
最大プーリングカーネルのサイズを$3 \times 3 , ~4 \times 4$のように変化させたとき実行結果がどのように変化するかを試した。

\subsection{LeNet-5の実装} \label{subsec : LeNet-5の実装}
節\ref{sec : 畳み込みニューラルネットワークのアーキテクチャ}で説明したアーキテクチャLeNet-5を実装する。層の名前は図\ref{fig : lenet-5}に準ずるものとする。入力層にゼロパディングを施して$32 \times 32$の画像にし、$5 \times 5$のフィルター$6$枚を持つ畳み込み層C1を構築した。それに$2 \times 2$のプーリングカーネルで最大プーリングを施した（S2）。それぞれの出力画像に対して$16$枚の$5 \times 5$のフィルターで畳み込みを行い（C3）、さらにその各出力画像について$2 \times 2$のプーリングカーネルで最大プーリングを施した（S4）。この時点で出力のサイズは$5 \times 5 \times 16$であるがこれを$120$次元のベクトルに展開した（C5）。次に$84$のユニットを持つ隠れ層F6を通してから出力層とした。最後の３つの層は全結合型のフィードフォーワード・ニューラルネットワークとみなせる。

これを実装したあと、畳み込み層C5とF6の間、F6と出力層の間、そのどちらにも$p = 0.25$のドロップアウトを挿入した場合について性能を確認した。

\subsection{畳み込みニューラルネットワークのアーキテクチャ} \label{subsec : 畳み込みニューラルネットワークのアーキテクチャ}
畳み込みニューラルネットワークにおいて層の数増やしていくのがいいのか、減らしていくのがいいのかを実験して検証した。節\ref{subsec : LeNet-5の実装}のLeNet-5のネットワークを用いて、畳み込みのフィルターを入力層から順に$16,~32,~64$とし、出力層の手前の層のユニット数を$128$にしたネットワークAと、それを逆にしたネットワークBについて実験を行なった（図\ref{fig : nn-a-b}）。エポック数は$15,~$ミニバッチのサイズは$1000$とした。
%１枚の画像
\begin{figure}[htbp]
\centering
\includegraphics[width=120mm]{fig-nn-a-b.png}
\caption{上図がネットワークA、下図がネットワークB}
\label{fig : nn-a-b}
\end{figure}





\end{document}