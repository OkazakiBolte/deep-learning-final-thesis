%\documentclass{jsarticle}
%\documentclass[draft]
%\documentclass[dvipdfmx,autodetect-engine,draft]{jsarticle}% autodetect-engine で pLaTeX / upLaTeX を自動判定
\documentclass[dvipdfmx,autodetect-engine]{jsarticle}% autodetect-engine で pLaTeX / upLaTeX を自動判定

\usepackage[dvipdfmx]{graphicx}
\usepackage{url}
\usepackage{bm}
\usepackage{comment}
%\usepackage{split}
\usepackage{multirow}
\usepackage{listings,jlisting}
\usepackage{braket}
\usepackage{physics}
\usepackage{xparse,amsmath}
\usepackage{here}
\usepackage{enumerate}
\usepackage{mathrsfs}
%\usepackage{jlistings} %日本語のコメントアウトをする場合jlistingが必要
\setcounter{tocdepth}{3}
\usepackage{amsmath,amssymb}

\usepackage{empheq}



%ここからソースコードの表示に関する設定
\lstset{
  basicstyle={\ttfamily},
  identifierstyle={\small},
  commentstyle={\smallitshape},
  keywordstyle={\small\bfseries},
  ndkeywordstyle={\small},
  stringstyle={\small\ttfamily},
  frame={tb},
  breaklines=true,
  columns=[l]{fullflexible},
  numbers=left,
  xrightmargin=0zw,
  xleftmargin=3zw,
  numberstyle={\scriptsize},
  stepnumber=1,
  numbersep=1zw,
  lineskip=-0.5ex
}
\renewcommand{\lstlistingname}{ソースコード}
%ここまでソースコードの表示に関する設定

%
% proof environment without \qed
%
\makeatletter
%\renewenvironment{proof}[1][\proofname]{\par
\newenvironment{Proof}[1][\Proofname]{\par
  \normalfont
  \topsep6\p@\@plus6\p@ \trivlist
  \item[\hskip\labelsep{\bfseries #1}\@addpunct{\bfseries.}]\ignorespaces
}{%
  \endtrivlist
}
%\renewcommand{\proofname}{証明}
%\renewcommand{\proofname}{Proof}
\newcommand{\Proofname}{証明}
%\newcommand{\Proofname}{Proof}
\makeatother
%
% \qed
%
\makeatletter
\def\BOXSYMBOL{\RIfM@\bgroup\else$\bgroup\aftergroup$\fi
  \vcenter{\hrule\hbox{\vrule height.85em\kern.6em\vrule}\hrule}\egroup}
\makeatother
\newcommand{\BOX}{%
  \ifmmode\else\leavevmode\unskip\penalty9999\hbox{}\nobreak\hfill\fi
  \quad\hbox{\BOXSYMBOL}}
%\renewcommand\qed{\BOX}
\newcommand\QED{\BOX}



\begin{document}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


%分数
\newcommand{\f}[2]{\frac{#1}{#2}}
%偏微分ド(partial defferential)
\newcommand{\pd}[2]{\frac{\partial #1}{\partial#2}}
%微分
\newcommand{\D}[2]{\frac{ \mathrm{d} #1}{\mathrm{d}#2}}
%かける10のなんとか乗
\newcommand{\E}[1]{ \times 10^{#1}}
%数式中の単位（空白付き）
\newcommand{\un}[1]{~{\rm #1}}
%立体の添え字
\newcommand{\sub}[2]{#1_{\rm #2}}
%立体
\newcommand{\R}[1]{{\rm #1}}
%かける×
\def\*{\times}
%displaystyle
\newcommand{\dps}[1]{\displaystyle{#1}}
%ref
\newcommand{\rf}[1]{\ref{#1}}
%label
\newcommand{\lb}[1]{\label{#1}}
%\begin{equation}
\def\be{\begin{equation}}
%\end{equation}
\def\ee{\end{equation}}
%数式内の日本語
\newcommand{\jp}[1]{\mbox{#1}}
%イコールを揃える時の呪文
\def\bea{\begin{eqnarray}}
\def\eea{\end{eqnarray}}
%式番号をつけたくないときには
\def\bea*{\begin{eqnarray*}}
\def\eea*{\end{eqnarray*}}
%インテグラル
\newcommand{\I}[4]{\int_{#1}^{#2} \, #3 \, {\rm d} #4}
%レフト・ライト
\def\l{\left}
\def\r{\right}
%ベクトルの太文字 with \usepackage{bm}
%\newcommand{\vct}[1]{\bm{#1}}
%グラディエント
%\newcommand{\grad}[1]{{\rm grad}\, #1}
%ダイバージェント
%\newcommand{\Div}[1]{{\rm div}\, #1}
%ローテーション
%\newcommand{\rot}[1]{{\rm rot}\, #1}
%分散
%\def\var{{\rm var}}
%共分散
%\def\Cov{{\rm Cov}}
%平均のかっこ

\def\T{\mathsf{T}}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



\begin{flushright}
発表予定日：2019年10月28日
\\慶應義塾大学理工学部物理学科\\岡崎健人
\end{flushright}
\begin{center}
{\Large 卒研ゼミ「深層学習」} 
\end{center}
%箇条書き
%\tableofcontents   %👈目次
%\newpage


%%%%%%%%%%%%%%%%%%%%%%%
%2.6節
\setcounter{section}{4}
\section{機械学習の基礎}
\setcounter{subsection}{1}
\subsection{容量，過剰適合，過少適合}
\subsubsection{ノーフリーランチ定理（要約）}
機械学習アルゴリズムは、有限の訓練用のデータセット（training dataset）からそこに潜む一般的なパターンを見つけ出すが、このことは論理的に妥当\footnote{論理学の文脈でvalidは妥当であると訳されるのが普通（翻訳ミス？）。妥当であるとは、たとえば「Aである。AならばBである。ゆえにBである」という演繹的推論は、前提が真であれば結論は必ず真であり、このことを妥当であるという。一方で「a$_1$はPである。a$_2$はPである。ゆえに（おそらく）すべてのaはPである」という帰納的推論において、前提が真であっても結論が真であるとは保障されず、妥当ではない。}でないように思われる。機械学習では確率的なパターンのみを吐き出すことでこの問題を部分的に回避している。つまり、たとえば「カラス$1$、カラス$2$、…、カラス$n$は黒い」というデータセットを与えれば、「ほとんどすべてのカラスは黒い」というほぼ正しいルールを得ることが約束される。

これでもまだ問題は残っている。ノーフリーランチ定理（no free lunch theorem, NFLT）は次のように表現される。「すべての評価関数に適用できる（他の方法よりも）効率のよいアルゴリズムは存在しない\cite{進化論的計算手法}」「コスト関数の極値を探索するあらゆるアルゴリズムは、全ての可能なコスト関数に適用した結果を平均すると同じ性能となる\cite{wiki}」「与えられた課題に独立などんな学習アルゴリズムの優劣判定方法はない。どのようなモデルが最も優れているのかは、問題の種類や、付随する情報によって決まる\cite{結局どのアルゴリズムが良いのか？}」など。つまり、{\bf すべての評価関数に対して平均したとき}に、普遍的で絶対的に最良なアルゴリズムなど存在せず、個々の問題の特徴を把握して、それに最適な機械学習アルゴリズムを理解・選択すべきであるということだ\footnote{定理の名称は英語の格言``There ain't no such thing as a free lunch.''（TANSTAAFLと略される）から。かつてアメリカのサルーンでは、「飲みにきた客にはランチを無料でふるまう」という宣伝文句が用いられていた。うまい話のように思えるが、ランチ代は酒代に上乗せされているだけだった\cite{tanstaafl}。タダ飯のような素敵なものなんて存在せず、結局代金は支払わなければならない。同じように「すべての目的関数に使える万能で性能の良いアルゴリズム」のような素敵なものは存在せず、データに対して前提条件を設けるという代金を支払わなければならない。}。ノーフリーランチ定理の証明は参考文献\cite{進化論的計算手法}を参照されたい。

\subsubsection{正則化}
結局のところノーフリーランチ定理は、特定のタスクに対してうまく機能するアルゴリズムを設計しなければならないということをほのめかしているが、これは学習アルゴリズムに特定の設計を組み込んで修正すれば解決する。

最小二乗法での関数の推定においては、データの個数よりも近似する曲線の次数のほうが大きいときや、係数のベクトル$\vb*{w}=\qty[w_0,w_1,\ldots,w_{n}]^\mathsf{T}$の成分の値が大きくなると、過学習に陥りやすくなる。このことを防ぐために平均二乗誤差$\mathrm{MSE}(\vb*{w})$に重み減衰$\lambda \vb*{w}^\T \vb*{w}$を加えたもの$J(\vb*{w})=\mathrm{MSE}\qty(\vb*{w}) + \lambda \vb*{w}^\T \vb*{w}$を最小化することを考える。データの数を$m,~$データを$(x_i,y_i),~$目的関数を$n$次多項式$\hat{y}=w_0+w_1 x + w_2 x^2 + \cdots + w_{n} x^{n}=\displaystyle\sum_{k=0}^{n} w_k x^{k}$とし、この係数$\vb*{w}$を求める。行ベクトル${\vb*{x}_i}^{\mathsf{T}} = \qty[1,{x_i},{x_i}^2,\ldots,{x_i}^{n}]$を縦に並べた行列
$$X=\mqty[ {\vb*{x}_1}^{\mathsf{T}} \\ {\vb*{x}_1}^{\mathsf{T}} \\ \vdots \\ {\vb*{x}_{m}}^{\mathsf{T}}]=\mqty[ 1 & x_1 & {x_1}^2 & \ldots & {x_1}^{n} \\ 1 & x_2 & {x_2}^2 & \ldots & {x_2}^{n} \\ & & \vdots & &  \\1 & x_{m} & {x_{m}}^2 & \ldots & {x_{m}}^{n} ]$$
と（これは計画行列（design matrix）というのだった。日本語版教科書\cite{dl1}のp. 77.）、ベクトル$\vb*{y}=\qty[y_1,\ldots,y_m]^\mathsf{T}$を用いれば、$\pdv*{(\vb*{w}^\T \vb*{w})}{\vb*{w}}=2\vb*{w}$より
$$\pdv{J(\vb*{w})}{\vb*{w}}=\pdv{\mathrm{MSE}(\vb*{w})}{\vb*{w}}+\pdv{\qty(\lambda \vb*{w}^\T \vb*{w} )}{\vb*{w}}= \frac{2}{m} \qty(X^\mathsf{T} X \vb*{w} - X^\mathsf{T} \vb*{y}) + 2\lambda \vb*{w}.$$
これを$\vb*{0}$とすれば、$\vb*{w}$を変数とみた$(n+1)$本の連立方程式$\qty(X^\T X +m \lambda I)\vb*{w}=X^\T \vb*{y}$が得られる。$\lambda$の値を大きくしていくと、連立方程式におけるすべての対角成分$w_i$の係数が大きくなってゆく。しかし右辺の$X^\T \vb*{y}$はデータから得られる定ベクトルであるため、$w_i$の値は小さくならなければならない。ゆえに目的関数は多項式の丸みを生かせなくなり、直線に近い形となる。逆に$\lambda$の値が小さいときは$w_i$の値が大きくなるため目的関数はよく曲がるような、柔軟な曲線となる。このことはデータ点をすべて曲線で無理やり繋いでしまうような、過学習の状態になることを示している。中程度の値の$\lambda$を用いれば、過少適合も過剰適合も回避できるようになる。\\

いま正則化項を$\lambda \norm{\vb*{w}}_2^2$としていたが、この線形回帰をリッジ回帰（Ridge regression）という。ほかにも正則化項を$\lambda \norm{\vb*{w}}_1^1$としたものや、$\norm{\vb*{w}}_2^2$と$\norm{\vb*{w}}_1^1$の線型結合としたものもあり、それぞれラッソ回帰（LASSO regression）、Elastic Netという\cite{aurelien}。正則化は最適化と並んで重要であるため、詳しくはまとめて第７章で勉強する。

\subsubsection*{なぜ$J(\vb*{w}) = \mathrm{MSE}(\vb*{w}) + \lambda \vb*{w}^\T \vb*{w}$か？}
目的関数を多項式とするときの係数のベクトル$\vb*{w}=\qty[w_0,w_1,\ldots,w_{n}]^\mathsf{T}$の成分が大きいと、目的関数の曲がり方が激しくなって、過学習に陥ってしまう。そこで$t$をある定数として、$\norm{\vb*{w}}_2^2 = \vb*{w}^\T \vb*{w} \le t$という制約のもとでの平均二乗誤差$\mathrm{MSE}(\vb*{w})=\frac{1}{m} \norm{X \vb*{w}-\vb*{y}}_2^2 = \frac{1}{m}  \qty(X\vb*{w}-\vb*{y})^\mathsf{T} \qty(X\vb*{w}-\vb*{y}) $の最小化を考える。これはKKT法を用いれば解ける。一般化ラグランジュ関数を
$$L(\vb*{w},\lambda)=\frac{1}{m}  \qty(X\vb*{w}-\vb*{y})^\mathsf{T} \qty(X\vb*{w}-\vb*{y})  + \lambda \qty(  \vb*{w}^\T \vb*{w}- t )$$
としてKKT条件は
\begin{empheq}[left={\empheqlbrace}]{align}
\pdv{L}{\vb*{w}} &= \frac{2}{m} \qty(X^\mathsf{T} X  \vb*{w} - X^\mathsf{T} \vb*{y}) + 2\lambda \vb*{w}= \vb*{0}  ,\nonumber \\
\pdv{L}{\lambda}&= \vb*{w}^\T \vb*{w}- t  \le 0, \nonumber \\
\lambda &  \qty( \vb*{w}^\T \vb*{w}- t ) = 0, \nonumber \\
\lambda &\ge 0 \nonumber 
\end{empheq}
となる。$\lambda = 0$の場合は最適な解$\vb*{w}$がもともと$\vb*{w}^\T \vb*{w} \le t$を満たしていたことを表す。もし$\lambda > 0$ならば$\vb*{w}^\T \vb*{w} = t$だが、$\vb*{w}$は第１式より$\vb*{w} = \qty( X^\T X + m \lambda I  )^{-1} X^\T \vb*{y}$である。したがって
$$t = \vb*{w}^\T \vb*{w} = \qty[\qty( X^\T X +  m \lambda I   )^{-1} X^\T \vb*{y}]^\T \qty( X^\T X +  m \lambda I   )^{-1} X^\T \vb*{y}.$$
これより$\lambda$の値を決めれば$t$の値は自動的に決まることがわかる\cite{wiki3}。したがって、$\lambda$をパラメータとして最初から一般化ラグランジュ関数$L(\vb*{w},\lambda)$から$-\lambda t$の項を省いた$J(\vb*{w}) = \mathrm{MSE}(\vb*{w}) + \lambda \vb*{w}^\T \vb*{w}$という関数の最小化をすればよい。


\begin{thebibliography}{9}
  \bibitem{進化論的計算手法} 伊藤斉志，「進化論的計算手法」，2005年．pp. 107--122.
  \bibitem{wiki}「ノーフリーランチ定理」，Wikipedia，\url{https://ja.wikipedia.org/wiki/}ノーフリーランチ定理, 最終閲覧\today
  \bibitem{結局どのアルゴリズムが良いのか？}浅川伸一，「結局どのアルゴリズムが良いのか？」，\url{https://www.cis.twcu.ac.jp/~asakawa/waseda2002/nofreelunch.pdf}
  \bibitem{tanstaafl} ``There's no such thing as a free lunch'', The Phrase Finder, \url{https://www.phrases.org.uk/meanings/tanstaafl.html}, 最終閲覧\today.
 \bibitem{dl1} Ian Goodfellow {\it et al}. 著，岩澤有祐ほか訳，「深層学習」，株式会社ドワンゴ，2018年．
 \bibitem{aurelien} Aur\'{e}lien G\'{e}ron, 長尾高弘訳．「scikkit--learnとTensorFlowによる実践機械学習」，株式会社オライリージャパン，2018年．pp. 128--133.
 \bibitem{wiki3} ``Lasso (statistiques)'', Wikip\'{e}dia, \url{https://fr.wikipedia.org/wiki/Lasso_(statistiques)}. 最終閲覧\today.
 \end{thebibliography}
\end{document}